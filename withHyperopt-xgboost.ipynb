{
 "metadata": {
  "name": "",
  "signature": "sha256:18493d337c2c642f64c335ee8ecb608867d4dc8174b1914d380e7040aaaaeae4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Use Hyperopt to identify best xgboost parameters "
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "\n",
      "test data format:\n",
      "EventId,DER_mass_MMC,DER_mass_transverse_met_lep,DER_mass_vis,DER_pt_h,DER_deltaeta_jet_jet,DER_mass_jet_jet,DER_prodeta_jet_jet,DER_deltar_tau_lep,DER_pt_tot,DER_sum_pt,DER_pt_ratio_lep_tau,DER_met_phi_centrality,DER_lep_eta_centrality,PRI_tau_pt,PRI_tau_eta,PRI_tau_phi,PRI_lep_pt,PRI_lep_eta,PRI_lep_phi,PRI_met,PRI_met_phi,PRI_met_sumet,PRI_jet_num,PRI_jet_leading_pt,PRI_jet_leading_eta,PRI_jet_leading_phi,PRI_jet_subleading_pt,PRI_jet_subleading_eta,PRI_jet_subleading_phi,PRI_jet_all_pt\n",
      "350000,-999.0,79.589,23.916,3.036,-999.0,-999.0,-999.0,0.903,3.036,56.018,1.536,-1.404,-999.0,22.088,-0.54,-0.609,33.93,-0.504,-1.511,48.509,2.022,98.556,0,-999.0,-999.0,-999.0,-999.0,-999.0,-999.0,-0.0\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import platform\n",
      "if platform.system() == 'Darwin':\n",
      "    xgboost_path = '/Users/andyh_mac/xgboost/xgboost-master/python'\n",
      "    number_threads = 8\n",
      "    data_dir = '/Users/andyh_mac/Desktop/Analytics/Higgs/data_std_2split/'\n",
      "elif platform.system() == 'Linux':\n",
      "    xgboost_path = '/home/ubuntu/xgboost-master/python'\n",
      "    number_threads = 32\n",
      "    data_dir = '/mnt2/Higgs/data_std_3split/'\n",
      "else:\n",
      "    print \"Don't know parameters for this system: %s\" % platform.system()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from hyperopt import fmin, rand, tpe, space_eval, hp, Trials\n",
      "from hyperopt import STATUS_OK, STATUS_FAIL\n",
      "from matplotlib import pyplot as plt\n",
      "import higgs_lib\n",
      "import hyper_lib\n",
      "import math\n",
      "import numpy as np\n",
      "import pickle\n",
      "from pymongo import MongoClient\n",
      "import sys\n",
      "sys.path.append(xgboost_path)\n",
      "import xgboost as xgb"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Pick a random seed for reproducible results. Choose wisely!\n",
      "np.random.seed(42)\n",
      "\n",
      "# Put Y(truth), X(data), W(weight), and I(index) into their own arrays\n",
      "print 'Assigning data to numpy arrays.'\n",
      "# First 80% are training\n",
      "Y_train = np.loadtxt( data_dir + 'Y_train_2.csv', delimiter=',', skiprows=1 )\n",
      "X_train = np.loadtxt( data_dir + 'X_train_2.csv', delimiter=',', skiprows=1 )\n",
      "W_train = np.loadtxt( data_dir + 'W_train_2.csv', delimiter=',', skiprows=1 )\n",
      "# Next 10% are validation\n",
      "Y_valid = np.loadtxt( data_dir + 'Y_valid_2.csv', delimiter=',', skiprows=1 )\n",
      "X_valid = np.loadtxt( data_dir + 'X_valid_2.csv', delimiter=',', skiprows=1 )\n",
      "W_valid = np.loadtxt( data_dir + 'W_valid_2.csv', delimiter=',', skiprows=1 )\n",
      "weight = W_train * (float(X_train.shape[0]) / len(Y_train))\n",
      "\n",
      "# Compute weight characteristics\n",
      "sum_wpos = sum( weight[i] for i in range(len(Y_train)) if Y_train[i] == 1.0  )\n",
      "sum_wneg = sum( weight[i] for i in range(len(Y_train)) if Y_train[i] == 0.0  )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Assigning data to numpy arrays.\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Optional - Make dummy vars for missing jets data and missing der_mass_mmc\n",
      "# Two new vars:\n",
      "# First one is one when DER_MASS_MMC is -999 and zero otherwise\n",
      "# Second one is one when PRI_JET_NUM is less than or equal to 1 and zero otherwise\n",
      "USE_DUMMIES = True\n",
      "if USE_DUMMIES == True:\n",
      "    X_train = np.hstack((X_train, higgs_lib.make_dummies(X_train)))\n",
      "    X_valid = np.hstack((X_valid, higgs_lib.make_dummies(X_valid)))\n",
      "\n",
      "# Format data for xgboost\n",
      "dtrain = xgb.DMatrix(X_train,label=Y_train, weight = weight)\n",
      "dvalid = xgb.DMatrix(X_valid,label=Y_valid)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Set up hyperopt parameter generation\n",
      "space =  [  hp.uniform('depth',3,8), hp.uniform('eta',.05,.5),hp.quniform('threshold',60,90,1)]\n",
      "\n",
      "# Define hyperopt objective function optimizing for AMS\n",
      "n_boost_iter=100  # n_boost_iters for each hyper param iter\n",
      "def objective(args):\n",
      "    depth,eta,threshold = args\n",
      "    threshold = int(threshold)\n",
      "    depth = int(depth)\n",
      "    param = {}\n",
      "    # use logistic regression loss, use raw prediction before logistic transformation\n",
      "    # since we only need the rank\n",
      "    param['objective'] = 'binary:logitraw'\n",
      "    # scale weight of positive examples\n",
      "    param['scale_pos_weight'] = sum_wneg/sum_wpos\n",
      "    param['bst:eta'] = eta \n",
      "    param['bst:max_depth'] = depth\n",
      "    param['eval_metric'] = 'auc'\n",
      "    param['silent'] = 0\n",
      "    param['nthread'] = number_threads\n",
      "    \n",
      "    # specify validations set to watch performance\n",
      "    evallist  = [(dvalid,'eval'), (dtrain,'train')]\n",
      "\n",
      "    # Train the GradientBoostingClassifier\n",
      "    bst = xgb.train( param, dtrain, n_boost_iter, evallist )\n",
      "\n",
      "    # Get the probaility output from the trained method, using the 10% for testing\n",
      "    predict_train = bst.predict(dtrain)\n",
      "    predict_valid = bst.predict(dvalid)\n",
      "\n",
      "    # Select a cutoff point for assign signal and background labels\n",
      "    pcut = np.percentile(predict_train,threshold)\n",
      "    \n",
      "    # These are the final signal and background predictions\n",
      "    Yhat_train = predict_train > pcut \n",
      "    Yhat_valid = predict_valid > pcut   \n",
      "    # Calc numeber of s and b TruePos and True Neg for training and validation\n",
      "    s_train, b_train = higgs_lib.count_s_b(W_train,Y_train,Yhat_train)\n",
      "    s_valid, b_valid = higgs_lib.count_s_b(W_valid,Y_valid,Yhat_valid)\n",
      "    trial_results={}\n",
      "    trial_results['loss'] = higgs_lib.inv_AMSScore(s_train,b_train)\n",
      "    trial_results['valid_loss'] = higgs_lib.inv_AMSScore(s_valid,b_valid)\n",
      "    trial_results['status'] = STATUS_OK\n",
      "    return trial_results"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Call hyperopt fmin to iterate over hyper parameters\n",
      "print 'Training classifier (this may take some time!)'\n",
      "hyperopt_iter = 30\n",
      "trials = Trials()\n",
      "best = fmin(objective, space, algo=rand.suggest, max_evals=hyperopt_iter, trials=trials)    \n",
      "print best"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training classifier (this may take some time!)\n",
        "{'threshold': 63.0, 'depth': 7.730887672290253, 'eta': 0.24252949436961502}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Plot AMS train vs validataion for choices of hyper parameters\n",
      "import hyper_lib\n",
      "hyper_lib.plot_trials(trials)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Inspect parameter details for nth hyper parameter combination\n",
      "reload(hyper_lib)\n",
      "hyper_lib.which_tid(2,trials)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[{'refresh_time': datetime.datetime(2014, 9, 22, 22, 21, 21, 681000), 'book_time': datetime.datetime(2014, 9, 22, 22, 20, 41, 424000), 'misc': {'tid': 2, 'idxs': {'threshold': [2], 'depth': [2], 'eta': [2]}, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'vals': {'threshold': [79.0], 'depth': [5.1790343711676465], 'eta': [0.06167373274896859]}, 'workdir': None}, 'state': 2, 'tid': 2, 'exp_key': None, 'version': 0, 'result': {'status': 'ok', 'loss': 1.0, 'valid_loss': 1}, 'owner': None, 'spec': None}]\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Best so far:\n",
      "using log & dummies\n",
      "{'threshold': [86.0], 'depth': [3.0615393522647496], 'eta': [0.2472423296083084]}\n",
      "{'threshold': [83.0], 'depth': [6.61242116667745], 'eta': [0.23325277514869436]}\n",
      "***\n",
      "using \n",
      "{'threshold': [82.0], 'depth': [5.7480279001901735], 'eta': [0.17001343752893355]} used for first stack.\n",
      "{'threshold': 87.0, 'depth': 6.4814738448731894, 'eta': 0.4109641832220093}\n",
      "{'threshold': 71.0, 'depth': 11.456747161913151, 'eta': 0.4561892980077648} 100 boost, 30 rand hyperopt\n",
      "{'depth': 10.781242360313211, 'eta': 0.49086662149306215, 'pcut': 86.0}\n",
      "{'eta': 0.49086662149306215, 'pcut': 86.0} iter =500, depth = 5\n",
      "{'depth': 15.0, 'eta': 0.2773768403503458, 'pcut': 62.0}\n",
      "iter = 500\n",
      "    "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reload(higgs_lib)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "<module 'higgs_lib' from 'higgs_lib.py'>"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Run xgboost with best parameters and inspect AMS\n",
      "n_boost_iter = 100\n",
      "threshold = 83\n",
      "eta = .233\n",
      "depth = 6\n",
      "\n",
      "# use logistic regression loss, use raw prediction before logistic transformation\n",
      "# since we only need the rank\n",
      "param={}\n",
      "param['objective'] = 'binary:logitraw'\n",
      "# scale weight of positive examples\n",
      "param['scale_pos_weight'] = sum_wneg/sum_wpos\n",
      "param['bst:eta'] = eta \n",
      "param['bst:max_depth'] = depth\n",
      "param['eval_metric'] = 'auc'\n",
      "param['silent'] = 0\n",
      "param['nthread'] = number_threads\n",
      "\n",
      "# specify validations set to watch performance\n",
      "evallist  = [(dvalid,'eval'), (dtrain,'train')]\n",
      "\n",
      "# Train\n",
      "bst = xgb.train( param, dtrain, n_boost_iter, evallist )\n",
      " \n",
      "# Predict \n",
      "predict_train = bst.predict(dtrain)\n",
      "predict_valid = bst.predict(dvalid)\n",
      "\n",
      "# Select a cutoff point for assign signal and background labels\n",
      "pcut = np.percentile(predict_train,threshold)\n",
      " \n",
      "# These are the final signal and background predictions\n",
      "Yhat_train = predict_train > pcut \n",
      "Yhat_valid = predict_valid > pcut\n",
      "\n",
      "# Calc numeber of s and b TruePos and True Neg for training and validation\n",
      "s_train, b_train = higgs_lib.count_s_b(W_train,Y_train,Yhat_train)\n",
      "s_valid, b_valid = higgs_lib.count_s_b(W_valid,Y_valid,Yhat_valid)\n",
      " \n",
      "# Now calculate the AMS scores\n",
      "print 'Calculating AMS score for a probability cutoff %s' % pcut\n",
      "print '   - AMS based on 90% training   sample:',higgs_lib.AMSScore(s_train,b_train)\n",
      "print '   - AMS based on 10% validation sample:',higgs_lib.AMSScore(s_valid,b_valid)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Calculating AMS score for a probability cutoff 1.06119668484\n",
        "   - AMS based on 90% training   sample: 2.85622989187\n",
        "   - AMS based on 10% validation sample: 2.97567821481\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Optionally write a submission csv file\n",
      "# Better submissions result from applying bagging first\n",
      "# Load the testing data\n",
      "print 'Loading testing data'\n",
      "data_test = np.loadtxt( 'test.csv', delimiter=',', skiprows=1 )\n",
      "X_test = data_test[:,1:31]\n",
      "I_test = list(data_test[:,0])\n",
      "\n",
      "if USE_DUMMIES == True:\n",
      "    # Make dummies\n",
      "    X_test = np.hstack((X_test, higgs_lib.make_dummies(X_test)))\n",
      "\n",
      "# Format data for xgboost\n",
      "dtest = xgb.DMatrix(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Loading testing data\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get a vector of the probability predictions which will be used for the ranking\n",
      "print 'Building predictions'\n",
      "Predictions_test = bst.predict(dtest)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Building predictions\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Assign labels based the best pcut\n",
      "Label_test = list(Predictions_test>pcut)\n",
      "Predictions_test =list(Predictions_test)\n",
      " \n",
      "# Now we get the CSV data, using the probability prediction in place of the ranking\n",
      "print 'Organizing the prediction results'\n",
      "resultlist = []\n",
      "for x in range(len(I_test)):\n",
      "    resultlist.append([int(I_test[x]), Predictions_test[x], 's'*(Label_test[x]==1.0)+'b'*(Label_test[x]==0.0)])\n",
      " \n",
      "# Sort the result list by the probability prediction\n",
      "resultlist = sorted(resultlist, key=lambda a_entry: a_entry[1]) \n",
      " \n",
      "# Loop over result list and replace probability prediction with integer ranking\n",
      "for y in range(len(resultlist)):\n",
      "    resultlist[y][1]=y+1\n",
      " \n",
      "# Re-sort the result list according to the index\n",
      "resultlist = sorted(resultlist, key=lambda a_entry: a_entry[0])\n",
      " \n",
      "# Write the result list data to a csv file\n",
      "print 'Writing a final csv file Kaggle_higgs_prediction_output.csv'\n",
      "fcsv = open('Kaggle_higgs_prediction_output.csv','w')\n",
      "fcsv.write('EventId,RankOrder,Class\\n')\n",
      "for line in resultlist:\n",
      "    theline = str(line[0])+','+str(line[1])+','+line[2]+'\\n'\n",
      "    fcsv.write(theline) \n",
      "fcsv.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Organizing the prediction results\n",
        "Writing a final csv file Kaggle_higgs_prediction_output.csv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Plot results for inspection"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.hist([predict_train],stacked=True, bins =30, range=[-10,8], color=[ 'Khaki'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "(array([     0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
        "            0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
        "        64423.,  29999.,  31052.,  39114.,  60441.,      0.,      0.,\n",
        "            0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
        "            0.,      0.]),\n",
        " array([-10. ,  -9.4,  -8.8,  -8.2,  -7.6,  -7. ,  -6.4,  -5.8,  -5.2,\n",
        "        -4.6,  -4. ,  -3.4,  -2.8,  -2.2,  -1.6,  -1. ,  -0.4,   0.2,\n",
        "         0.8,   1.4,   2. ,   2.6,   3.2,   3.8,   4.4,   5. ,   5.6,\n",
        "         6.2,   6.8,   7.4,   8. ]),\n",
        " <a list of 30 Patch objects>)"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.hist([Predictions_test],stacked=True, bins =30, color=[  'DarkOrange'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "(array([ 85025.,  32772.,  10841.,  15498.,  12360.,  18621.,   9169.,\n",
        "        15640.,   9582.,   7927.,   9346.,   8290.,  17700.,  10720.,\n",
        "        11171.,   7427.,  11791.,  14152.,  11460.,  15885.,  11876.,\n",
        "        16734.,  19256.,  16720.,  17610.,  16156.,  24490.,  23593.,\n",
        "        22705.,  45483.]),\n",
        " array([-1.48281407, -1.38818968, -1.29356529, -1.1989409 , -1.1043165 ,\n",
        "       -1.00969211, -0.91506772, -0.82044333, -0.72581894, -0.63119454,\n",
        "       -0.53657015, -0.44194576, -0.34732137, -0.25269698, -0.15807258,\n",
        "       -0.06344819,  0.0311762 ,  0.12580059,  0.22042499,  0.31504938,\n",
        "        0.40967377,  0.50429816,  0.59892255,  0.69354695,  0.78817134,\n",
        "        0.88279573,  0.97742012,  1.07204452,  1.16666891,  1.2612933 ,\n",
        "        1.35591769]),\n",
        " <a list of 30 Patch objects>)"
       ]
      }
     ],
     "prompt_number": 13
    }
   ],
   "metadata": {}
  }
 ]
}